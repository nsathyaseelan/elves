#test-data-integrity.yml
# Description: Verify the Data Integrity of cloned volumes using FIO
# Author: nsathyaseelan

###############################################################################################
#Test Steps:
# 1. Check the maya-apiserver is running.
# 2. Download and Copy Test artifacts to kubemaster.
# 3. Check the snapshot-controller is running.
# 4. Create test artifacts Namespace.
# 5. Run the FIO Write job on OpenEBS volumes.
# 6. Check the status of FIO job.
# 7. Create the snapshot of OpenEBS volumes.
# 8. Create new PVC from snapshot.
# 9. Check the Synchronizing.
#10. Check the status of crtl and replica.
#11. Run the FIO Read job on cloned volumes.
#12. Verify the Data integrity on Cloned volumes.
#13. Cleaning the test artifacts.
###############################################################################################

---
- hosts: localhost

  vars_files:
    - test-di-vars.yml

  tasks:

    - block:

        - include: test-di-prereq.yml

        - name: 1) Check if maya-apiserver is running.
          include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
          vars:
            ns: openebs
            app: apiserver

        - name: 2) Copy the snapshot specific files to K8s master
          copy:
            src: "{{ item }}"
            dest: "{{ result_kube_home.stdout }}"
          with_items: "{{ snapshot_files }}"
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 2a) Copy the fio job specific files to K8s master
          copy:
            src: "{{ item }}"
            dest: "{{ result_kube_home.stdout }}"
          with_items: "{{ fio_files }}"
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 3) Check if snapshot-controller is running.
          include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
          vars:
            ns: openebs
            app: openebs-snapshot-controller

        - name: 4) Create test specific namespace
          shell: source ~/.profile; kubectl create ns {{ namespace }}
          args:
            executable: /bin/bash
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 5) Deploy the fio write jobs yaml
          shell: source ~/.profile; kubectl create -f "{{ fio_write }}" -n "{{ namespace }}"
          args:
            executable: /bin/bash
          register: result_fio
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 6) Check if fio write job is completed
          shell: source ~/.profile; kubectl get pods -n "{{ namespace }}" | grep fio
          args:
            executable: /bin/bash
          register: result_fio_pod
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Completed' in result_fio_pod.stdout"
          delay: 60
          retries: 10

        - name: 6a) Check if fio job created successfully
          shell: source ~/.profile; kubectl get jobs -n "{{ namespace }}"
          args:
            executable: /bin/bash
          register: result_job
          failed_when: "'fio' not in result_job.stdout"
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 7) Create the volume snapshot
          shell: source ~/.profile; kubectl create -f "{{ snapshot }}"
          args:
            executable: /bin/bash
          register: result_snap
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: '"created" in result_snap.stdout'
          delay: 60
          retries: 5

        - name: 7a) Check if the snapshot is created successfully
          shell: >
             source ~/.profile;
             kubectl describe volumesnapshot snapshot-demo -n "{{ namespace }}"
             | grep Message
          args:
            executable: /bin/bash
          register: result_snapout
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Snapshot created successfully' in result_snapout.stdout"
          delay: 60
          retries: 5

        - name: 8) Creating PVC from the snapshot
          shell: source ~/.profile; kubectl create -f "{{ snapshot_claim }}"
          args:
            executable: /bin/bash
          register: result_claim
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'created' in result_claim.stdout"
          delay: 60
          retries: 5

        - name: 8a) Checking if the pvc is created successfully
          shell: >
             source ~/.profile; kubectl get pvc -n "{{ namespace }}"
             | grep demo-snap-vol-claim
          args:
            executable: /bin/bash
          register: result_claim_pvc
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Bound' in result_claim_pvc.stdout"
          delay: 60
          retries: 10

        - name: 8b) Obtaining new PV name
          shell: >
           source ~/.profile; kubectl get pv -n "{{ namespace }}"
           | grep demo-snap-vol-claim | awk {'print $1'}
          args:
            executable: /bin/bash
          register: result_snap_pv
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 8c) Obtaining the new controller pod name
          shell: >
            source ~/.profile; kubectl get po -n "{{ namespace }}"
            | grep {{ result_snap_pv.stdout }} | grep ctrl | awk {'print $1'}
          args:
            executable: /bin/bash
          register: result_new_ctrl
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 8d) Obtaining the controller container name
          shell: >
           kubectl get pods {{ result_new_ctrl.stdout }} -n {{ namespace}}
           -o jsonpath='{.spec.containers[*].name}' | awk '{print $1}'
          args:
            executable: /bin/bash
          register: result_container
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 9) Checking if the sync completed
          shell: >
           source ~/.profile; kubectl logs {{ result_new_ctrl.stdout }}
           -c {{ result_container.stdout }} -n {{ namespace }}
          args:
            executable: /bin/bash
          register: result_ctrl_log
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Done synchronizing' in result_ctrl_log.stdout"
          delay: 100
          retries: 10

        - name: 10) Check if the ctrl and replica pods are running
          shell: >
           kubectl get pods -n "{{ namespace }}"
           | grep {{result_snap_pv.stdout}} | grep "{{ item }}"
          args:
            executable: /bin/bash
          register: result_pod_status
          until: "'Running' in result_pod_status.stdout"
          delay: 100
          retries: 15
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          with_items:
            - ctrl
            - rep

        - name: 11) Run fio read job with the snapped pvc
          include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_task.yml"
          vars:
           app_yml: "{{ fio_read }}"
           ns: "{{ namespace }}"

        - name: 11a) Check if fio read job is completed
          shell: source ~/.profile; kubectl get pods -n "{{ namespace }}" | grep read
          args:
            executable: /bin/bash
          register: result_fio_read
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
          until: "'Completed' in result_fio_read.stdout"
          delay: 60
          retries: 10

        - name: 11b) Obtaining the fio read job pod name
          shell: >
           source ~/.profile; kubectl get pods -n "{{ namespace }}"
           | grep read | awk '{ print $1 }'
          args:
            executable: /bin/bash
          register: result_read_pod
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: 12) Verify the data integrity check
          shell: >
           kubectl logs -f "{{ result_read_pod.stdout }}" -n "{{ namespace }}" | grep err
          args:
            executable: /bin/bash
          register: result_di
          failed_when: "'err= 0' not in  result_di.stdout"
          delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

        - name: Set the pass flag
          set_fact:
            flag: "Pass"

      rescue:
        - name: set the fail flag
          set_fact:
            flag: "Fail"

      always:
        - block:

            - name: 13) Cleaning the test artifacts
              include: test-di-cleanup.yml

            - name: Setting cleanup flag to passed
              set_fact:
                cflag: "Cleanup passed"

          rescue:
            - name: Setting cleanup tag to failed
              set_fact:
                cflag: "Cleanup failed"

          always:

            - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/stern_task.yml"
              vars:
                status: stop

            - name: Send slack notification
              slack:
                token: "{{ lookup('env','SLACK_TOKEN') }}"
                msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }}, {{ cflag }}'
              when: slack_notify | bool and lookup('env','SLACK_TOKEN')
